###### Import libraries #######
import os
from os import path
from glob import glob
import itertools
import cooler
import numpy as np
import pandas as pd
import bioframe
import toolz
from cooltools.lib.io import read_viewframe_from_file
from scipy import spatial


localrules:
    all,
    make_pentads,
    make_diff_pentads,
    merge_dots_across_resolutions,
    make_differential_insulation,
    make_tads,
    call_dots_mustache,
    get_bed_data,
    get_cool_data,


###### Read config parameters #######

project_folder = config.get("project_folder", "results")
inputs_folder = config.get("inputs_folder", "inputs")
coolers_folder = config.get("coolers_folder", path.join(inputs_folder, "coolers"))
beds_folder = path.normpath(config.get("beds_folder", path.join(inputs_folder, "beds")))
beds_folder_name = path.basename(beds_folder)
bedpes_folder = path.normpath(
    config.get("bedpes_folder", path.join(inputs_folder, "bedpes"))
)
bedpes_folder_name = path.basename(bedpes_folder)
expected_folder = path.normpath(
    config.get("expected_folder", path.join(project_folder, "expected"))
)
pileups_folder = path.normpath(
    config.get("pileups_folder", path.join(project_folder, "pileups"))
)
eigenvectors_folder = path.normpath(
    config.get("eigenvectors_folder", path.join(project_folder, "eigenvectors"))
)
saddles_folder = path.normpath(
    config.get("saddles_folder", path.join(project_folder, "saddles"))
)
insulation_folder = path.normpath(
    config.get("insulation_folder", path.join(project_folder, "insulation"))
)
tad_folder = path.normpath(config.get("tad_folder", path.join(project_folder, "tads")))
tad_folder_name = path.basename(tad_folder)
loop_folder = path.normpath(config.get("dot_folder", path.join(project_folder, "dots")))
loop_folder_name = path.basename(loop_folder)
boundary_folder = path.normpath(
    config.get("boundary_folder", path.join(project_folder, "boundaries"))
)
boundary_folder_name = path.basename(boundary_folder)


include: "rules/common.smk"


### Input genome
genome = config["genome"]
chroms = pd.read_csv(
    f'{config["chromsizes"]}', sep="\t", names=["chrom", "start", "end"]
)["chrom"].values

### Input cool files
from urllib.parse import urlparse


def will_download(link):
    parsed_path = urlparse(link)
    return False if parsed_path.scheme == "" else True


samples_df = pd.read_csv(
    config["samples"], sep="\t", header=0, comment="#", dtype={"do_dots": bool}
)
samples_df = samples_df.fillna(value=False)
samples_df.loc[:, "will_download"] = samples_df.file.apply(will_download)
samples_df.loc[:, "local_path"] = samples_df.apply(
    lambda x: f"{coolers_folder}/{x['sample']}.mcool" if x.will_download else x["file"],
    axis=1,
)
samples_df = samples_df.set_index("sample")

samples = list(samples_df.index)
coollinks_dict = dict(
    samples_df.query("will_download")["file"]
)  # dict with cools to be downloaded
coolfiles_dict = dict(samples_df["local_path"])

for coolfile in coolfiles_dict.keys():
    if coolfile not in coollinks_dict.keys():
        verify_view_cooler(
            cooler.Cooler(f"{coolfile}::resolutions/{config['resolutions'][0]}")
        )

### Input bed files
def get_files(folder, extension):
    files = list(map(path.basename, glob(f"{folder}/*{extension}")))
    return files


def make_local_path(bedname, kind):
    if kind == "bed":
        return f"{beds_folder}/{bedname}.bed"
    elif kind == "bedpe":
        return f"{bedpes_folder}/{bedname}.bedpe"
    else:
        raise ValueError("Only bed and bedpe file types are supported")


bedfiles_local = get_files(beds_folder, "bed")
bedpefiles_local = get_files(bedpes_folder, "bedpe")

local_bed_names = {
    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local
}
local_bedpe_names = {
    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"
    for bedpefile in bedpefiles_local
}

bed_df = pd.read_csv(config["annotations"], sep="\t", header=0, comment="#")
bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)
bed_df.loc[:, "local_path"] = bed_df.apply(
    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,
    axis=1,
    result_type="reduce",
)
bed_df = bed_df.set_index("bedname").replace("-", np.nan)

pileup_params = config["pileups"]["arguments"]

bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()

bedlinks_dict = dict(
    bed_df.loc[bed_df["will_download"], "file"]
)  # dict with beds to be downloaded
bedfiles_dict = dict(bed_df["local_path"])
bedfiles_dict.update(local_bed_names)
bedfiles_dict.update(local_bedpe_names)
bedfiles = list(bedfiles_dict.keys())
# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, 'pileups']]
bedtype_dict = dict(bed_df["format"])
# bedpe_pileups_mindist, bedpe_pileups_maxdist = config['bedpe_pileups_distance_limits']

samples_annotations = ~pd.read_csv(
    config["samples_annotations_combinations"],
    sep="\t",
    header=0,
    index_col=0,
    comment="#",
).isna()
### Data resolutions

if config["eigenvector"]["do"]:
    eigenvector_resolutions = config["eigenvector"]["resolutions"]

if config["saddle"]["do"]:
    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]
    saddle_mindists = [
        int(saddle_mindist * 2**i)
        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))
    ]
    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]

if config["pileups"]["do"]:
    shifts = config["pileups"]["shifts"]
    pileup_norms = []
    if shifts > 0:
        pileup_norms.append(f"{shifts}-shifts")
    if config["pileups"]["expected"]:
        pileup_norms.append("expected")
    if len(pileup_norms) == 0:
        raise ValueError("Please use expected or shifts to normalize pileups")
    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]
    pileup_resolutions = config["pileups"]["resolutions"]
    mindists = [
        int(pileups_mindist * 2**i)
        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))
    ]
    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]


if config["insulation"]["do"] and config["call_TADs"]["do"]:
    config["insulation"]["resolutions"] = merge_dicts(
        config["insulation"]["resolutions"], config["call_TADs"]["resolutions"]
    )
elif config["call_TADs"]["do"]:
    config["insulation"]["resolutions"] = config["call_TADs"]["resolutions"]

if config["insulation"]["do"]:
    insul_res_win_comb = []
    for resolution in config["insulation"]["resolutions"]:
        for win in config["insulation"]["resolutions"][resolution]:
            insul_res_win_comb.append(f"{resolution}_{win}")

if config["call_TADs"]["do"]:
    tad_res_win_comb = []
    for resolution in config["call_TADs"]["resolutions"]:
        for win in config["call_TADs"]["resolutions"][resolution]:
            tad_res_win_comb.append(f"{resolution}_{win}")

# chroms = cooler.Cooler(f'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}').chroms()[:]

# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]
# bedpe_separations = [f'{mindist}-{mindist*2}' for mindist in bedpe_mindists]

###### Setup outputs for the pipeline
# Expected files
expecteds = []
if config["expected"]["do"]:
    if config["expected"]["cis"]:
        expecteds += expand(
            f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",
            sample=samples,
            resolution=config["expected"]["resolutions"],
        )
    if config["expected"]["trans"]:
        expecteds += expand(
            f"{expected_folder}/{{sample}}_{{resolution}}.expected.trans.tsv",
            sample=samples,
            resolution=config["expected"]["resolutions"],
        )

# Insulation, boundaries, TADs and their pileups
insulation = (
    expand(
        f"{insulation_folder}/{{sample}}_{{resolution}}.insulation.tsv",
        sample=samples,
        resolution=config["insulation"]["resolutions"].keys(),
    )
    if (config["insulation"]["do"] or config["call_TADs"]["do"])
    else []
)

boundaries = (
    expand(
        f"{boundary_folder}/Boundaries_{{sample}}_{{resolution_window}}.bed",
        sample=samples,
        resolution_window=insul_res_win_comb,
    )
    if (config["insulation"]["do"] or config["call_TADs"]["do"])
    else []
)

boundaries_pileups = (
    expand(
        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Boundaries_{{sample}}_{{resolution_window}}_{{norm}}_local.clpy",
        sample=samples,
        resolution=pileup_resolutions,
        resolution_window=insul_res_win_comb,
        norm=pileup_norms,
    )
    if (config["insulation"]["do"] and config["insulation"]["pileup"])
    else []
)

diff_boundaries = (
    expand(
        f"{boundary_folder}/Insulation_{config['compare_boundaries']['samples'][0]}_not_"
        f"{config['compare_boundaries']['samples'][1]}_{{insul_res_win}}.bed",
        insul_res_win=insul_res_win_comb,
    )
    if config["compare_boundaries"]["do"]
    else []
)
diff_boundaries_pileups = (
    expand(
        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config['compare_boundaries']['samples'][0]}_not_"
        f"{config['compare_boundaries']['samples'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",
        sample=samples,
        resolution=pileup_resolutions,
        insul_res_win=insul_res_win_comb,
        norm=pileup_norms,
    )
    if config["compare_boundaries"]["do"] and config["compare_boundaries"]["pileup"]
    else []
)

tads = (
    expand(
        f"{tad_folder}/TADs_{{sample}}_{{tad_res_win}}.bed",
        sample=samples_df.index[samples_df["do_tads"]],
        tad_res_win=tad_res_win_comb,
    )
    if config["call_TADs"]["do"]
    else []
)


tads_pileups = (
    expand(
        f"{pileups_folder}/{tad_folder_name}/{{sample}}-{{resolution}}_over_TADs_{{sampleTADs}}_{{tad_res_win}}_{{norm}}_local_rescaled.clpy",
        sample=samples,
        resolution=pileup_resolutions,
        sampleTADs=samples_df.index[samples_df["do_tads"]],
        tad_res_win=tad_res_win_comb,
        norm=pileup_norms,
    )
    if config["call_TADs"]["do"] and config["call_TADs"]["pileup"]
    else []
)

# Dots and dot pileups
dot_methods = [
    m for m in config["call_dots"]["methods"] if config["call_dots"]["methods"][m]["do"]
]
if dot_methods:
    dots = expand(
        f"{loop_folder}/merged_resolutions/Dots_{{method}}_{{sampleDots}}.bedpe",
        method=dot_methods,
        sampleDots=samples_df.index[samples_df["do_dots"]],
    )

    dots_pileups = (
        expand(
            f"{pileups_folder}/{loop_folder_name}/{{sample}}-{{resolution}}_over_Dots_{{method}}_{{sampleDots}}_{{norm}}_{{mode}}.clpy",
            sample=samples,
            resolution=pileup_resolutions,
            method=dot_methods,
            sampleDots=samples_df.index[samples_df["do_dots"]],
            norm=pileup_norms,
            mode=["distal", "by_distance"],
        )
        if config["call_dots"]["pileup"]
        else []
    )
else:
    dots = []
    dots_pileups = []

# Setup file types for dot bedpes files and boundary/TAD beds
for file in dots:
    name = path.splitext(path.basename(file))[0]
    bedfiles_dict[name] = file
    bedtype_dict[name] = "bedpe"
for file in tads + diff_boundaries:
    name = path.splitext(path.basename(file))[0]
    bedfiles_dict[name] = file
    bedtype_dict[name] = "bed"

# Pileups for annotation files
beds_pileups = []
if config["pileups"]["do"]:
    for bedname, row in bed_df.iterrows():
        modes = []
        for mode in pileup_params.keys():
            if row[mode]:
                modes += [mode]

        for sample in samples:
            if sample not in samples_annotations.index:
                continue
            if (
                bedname in samples_annotations.columns
                and not samples_annotations.loc[sample, bedname]
            ):
                continue
            beds_pileups += expand(
                f"{pileups_folder}/{beds_folder_name}/{sample}-{{resolution}}_over_{bedname}_{{norm}}_{{mode}}.clpy",
                resolution=pileup_resolutions,
                norm=pileup_norms,
                mode=modes,
            )

# Saddles and compartment beds
saddles = (
    expand(
        f"{saddles_folder}/{{sample}}_{{resolution}}_over_{{sample}}_eig_{{bins}}{{dist}}.{{ending}}",
        sample=samples,
        resolution=eigenvector_resolutions,
        bins=config["saddle"]["bins"],
        dist=saddle_separations + [""],
        ending=["saddledump.npz", "digitized.tsv"],
    )
    if config["saddle"]["do"]
    else []
)
for sample1, sample2 in sample_pairs:
    saddles += (
        expand(
            f"{saddles_folder}/{sample1}_{{resolution}}_over_{sample2}_eig_{{bins}}{{dist}}.{{ending}}",
            resolution=eigenvector_resolutions,
            bins=config["saddle"]["bins"],
            dist=saddle_separations + [""],
            ending=["saddledump.npz", "digitized.tsv"],
        )
        if config["saddle"]["do"]
        else []
    )

compartments = (
    expand(
        f"{eigenvectors_folder}/compartments/{{sample}}_{{resolution}}_compartments.{{mode}}.bed",
        sample=samples,
        resolution=eigenvector_resolutions,
        mode=["cis"],  # Add option to do trans in the future
    )
    if config["eigenvector"]["save_compartment_beds"]
    else []
)


# Pentads for all samples based on compartments from all samples - for now all combinations!
pentads = (
    expand(
        f"{eigenvectors_folder}/pentads/{{sample}}_{config['pentads']['data_resolution']}_over_compartments_{{sample}}_{config['pentads']['eigenvector_resolution']}_{{norm}}_pentads.clpy",
        sample=samples,
        norm=config["pentads"]["norms"],
        mode=config["pentads"]["modes"],  # Add option to do trans in the future
    )
    if config["pentads"]["do"]
    else []
)
for sample1, sample2 in [pair.split("_vs_") for pair in sample_pairs]:
    pentads += (
        expand(
            f"{eigenvectors_folder}/pentads/{sample1}_{config['pentads']['data_resolution']}_over_compartments_{sample2}_{config['pentads']['eigenvector_resolution']}_{{norm}}_pentads.clpy",
            norm=config["pentads"]["norms"],
            mode=config["pentads"]["modes"],  # Add option to do trans in the future
        )
        if config["pentads"]["do"] and config["pentads"]["do_diff"]
        else []
    )

# Differential pentads calculated using compartments from sample2
diff_pentads = []
if config["pentads"]["do_diff"]:
    for sample_pair in sample_pairs:
        sample1, sample2 = sample_pair.split("_vs_")
        diff_pentads += expand(
            f"{eigenvectors_folder}/pentads/diff/{sample_pair}_{config['pentads']['data_resolution']}_over_compartments_{sample2}_{config['pentads']['eigenvector_resolution']}_{{norm}}_pentads.clpy",
            norm=config["pentads"]["norms"],
        )


# Pentads for all samples based on compartments from all samples - for now all combinations!
pentads = (
    expand(
        f"{eigenvectors_folder}/pentads/{{sample}}_{config['pentads']['data_resolution']}_over_compartments_{{sample}}_{config['pentads']['eigenvector_resolution']}_{{norm}}_pentads.clpy",
        sample=samples,
        norm=config["pentads"]["norms"],
        mode=config["pentads"]["modes"],  # Add option to do trans in the future
    )
    if config["pentads"]["do"]
    else []
)
for sample1, sample2 in sample_pairs:
    pentads += (
        expand(
            f"{eigenvectors_folder}/pentads/{sample1}_{config['pentads']['data_resolution']}_over_compartments_{sample2}_{config['pentads']['eigenvector_resolution']}_{{norm}}_pentads.clpy",
            norm=config["pentads"]["norms"],
            mode=config["pentads"]["modes"],  # Add option to do trans in the future
        )
        if config["pentads"]["do"] and config["pentads"]["do_diff"]
        else []
    )

# Differential pentads calculated using compartments from sample2
diff_pentads = []
if config["pentads"]["do_diff"]:
    for sample1, sample2 in sample_pairs:
        diff_pentads += expand(
            f"{eigenvectors_folder}/pentads/diff/{sample1}_vs_{sample2}_{config['pentads']['data_resolution']}_over_compartments_{sample2}_{config['pentads']['eigenvector_resolution']}_{{norm}}_pentads.clpy",
            norm=config["pentads"]["norms"],
        )


###### Define rules #######
rule all:
    input:
        lambda wildcards: expecteds,
        lambda wildcards: diff_boundaries,
        lambda wildcards: diff_boundaries_pileups,
        lambda wildcards: tads,
        lambda wildcards: tads_pileups,
        lambda wildcards: dots,
        lambda wildcards: dots_pileups,
        lambda wildcards: beds_pileups,
        lambda wildcards: saddles,


rule make_pileups:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        features=lambda wildcards: bedfiles_dict[wildcards.features],
        expected=lambda wildcards: f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv"
        if wildcards.norm == "expected"
        else [],
        view=lambda wildcards: config["view"],
    output:
        f"{pileups_folder}/{{folder}}/{{sample}}-{{resolution,[0-9]+}}_over_{{features}}_{{norm}}_{{extra,.*}}.clpy",
    wildcard_constraints:
        norm="(expected|nonorm|[0-9]+\-shifts)",
    params:
        features_format=lambda wildcards: bedtype_dict[wildcards.features],
        extra=lambda wildcards: f"{pileup_params[wildcards.extra]} {get_shifts(wildcards.norm)}",
    threads: 4
    resources:
        mem_mb=lambda wildcards, threads: threads * 16 * 1024,
        runtime=24 * 60,
    wrapper:
        "v1.19.1/bio/coolpuppy"


def dedup_dots(dots, hiccups_filter=False):
    newdots = []
    ress = list(sorted(set(dots["res"])))
    for chrom in sorted(set(dots["chrom1"])):
        chromdots = (
            dots[dots["chrom1"] == chrom]
            .sort_values(["start1", "start2"])
            .reset_index(drop=True)
        )
        for res in ress:
            chromdots["Supported_%s" % res] = chromdots["res"] == res
        # TODO fix!
        tree = spatial.cKDTree(chromdots[["start1", "start2"]])
        drop = []
        for i, j in tree.query_pairs(r=20000):
            ires = chromdots.at[i, "res"]
            jres = chromdots.at[j, "res"]
            chromdots.at[j, "Supported_%s" % ires] = True
            chromdots.at[i, "Supported_%s" % jres] = True
            if ires == jres:
                continue
            elif ires > jres:
                # if ress[-1] in (ires, jres) or abs(chromdots.at[j, 'start1']-chromdots.at[i, 'start1'])<=20000:
                drop.append(i)
            else:
                drop.append(j)
        newdots.append(chromdots.drop(drop))
    deduped = pd.concat(newdots).sort_values(["chrom1", "start1", "start2"])
    if hiccups_filter:
        l = len(deduped)
        deduped = deduped[
            ~(
                (deduped["start2"] - deduped["start1"] > 100000)
                & (~np.any(deduped[["Supported_%s" % res for res in ress[1:]]], axis=1))
            )
        ]
        print(
            l - len(deduped),
            "loops filtered out as unreliable %s resolution calls" % ress[0],
        )
    return deduped


def read_dots(f):
    df = pd.read_table(f, index_col=False, header=0).dropna(axis=1)
    res = int(f.split("_")[-1].split(".")[0])
    df["res"] = res
    return df


rule merge_dots_across_resolutions:
    input:
        dots=lambda wildcards,: [
            f"{loop_folder}/Loops_{{method}}_{{sample}}_{resolution}.bedpe"
            for resolution in config["call_dots"]["resolutions"]
        ],
    output:
        f"{loop_folder}/merged_resolutions/Loops_{{method}}_{{sample}}.bedpe",
    threads: 1
    resources:
        mem_mb=lambda wildcards, threads: 1024,
        runtime=5,
    run:
        dots = pd.concat((map(read_dots, input.dots))).reset_index(drop=True)
        dots = dedup_dots(dots)[
            ["chrom1", "start1", "end1", "chrom2", "start2", "end2"]
        ]
        dots.to_csv(output[0], sep="\t", header=False, index=False)


rule call_loops_cooltools:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        expected=f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",
        view=lambda wildcards: config["view"],
    output:
        f"{loop_folder}/Loops_cooltools_{{sample}}_{{resolution,[0-9]+}}.bedpe",
    threads: 4
    params:
        extra=lambda wildcards: config["call_dots"]["methods"]["cooltools"]["extra"],
    resources:
        mem_mb=lambda wildcards, threads: threads * 16 * 1024,
        runtime=24 * 60,
    wrapper:
        "v1.19.1/bio/cooltools/dots"


rule call_loops_chromosight:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
    output:
        bedpe=f"{loop_folder}/Loops_chromosight_{{sample}}_{{resolution,[0-9]+}}.bedpe",
        json=f"{loop_folder}/Loops_chromosight_{{sample}}_{{resolution,[0-9]+}}.json",
    threads: 4
    resources:
        mem_mb=lambda wildcards, threads: threads * 16 * 1024,
        runtime=24 * 60,
    conda:
        "envs/chromosight_env.yml"
    shell:
        f"chromosight detect --pattern loops --no-plotting -t {{threads}} {{input.cooler}}::resolutions/{{wildcards.resolution}} {loop_folder}/Loops_chromosight_{{wildcards.sample}}_{{wildcards.resolution}} && "
        f"mv {loop_folder}/Loops_chromosight_{{wildcards.sample}}_{{wildcards.resolution}}.tsv {{output.bedpe}}"


rule call_loops_mustache:
    input:
        f"{loop_folder}/Loops_mustache_{{sample}}_{{resolution}}.bedpe_tmp",
    output:
        f"{loop_folder}/Loops_mustache_{{sample}}_{{resolution,[0-9]+}}.bedpe",
    shell:
        """TAB=$(printf '\t') && cat {input} | sed "1s/.*/chrom1${{TAB}}start1${{TAB}}end1${{TAB}}chrom2${{TAB}}start2${{TAB}}end2${{TAB}}FDR${{TAB}}detection_scale/" > {output}"""


rule _call_loops_mustache:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
    output:
        f"{loop_folder}/Loops_mustache_{{sample}}_{{resolution,[0-9]+}}.bedpe_tmp",
    threads: 4
    params:
        args=config["call_dots"]["methods"]["mustache"]["extra"],
        dist=config["call_dots"]["methods"]["mustache"]["max_dist"],
    resources:
        mem_mb=lambda wildcards, threads: threads * 16 * 1024,
        runtime=24 * 60,
    conda:
        "envs/mustache_env.yml"
    shell:
        f"python3 -m mustache -p {{threads}} -f {{input.cooler}} -r {{wildcards.resolution}} "
        f"-d {{params.dist}} {{params.args}} -o {{output}}"


rule make_differential_insulation:
    input:
        insulation_WT=(
            f"{insulation_folder}/{{sampleWT}}_{{resolution}}.insulation.tsv"
        ),
        insulation_KO=(
            f"{insulation_folder}/{{sampleKO}}_{{resolution}}.insulation.tsv"
        ),
    output:
        f"{boundary_folder}/Insulation_{{sampleWT}}_not_{{sampleKO}}_{{resolution,[0-9]+}}_{{window,[0-9]+}}.bed",
    threads: 1
    resources:
        mem_mb=1024,
        runtime=60,
    run:
        insWT = pd.read_csv(input.insulation_WT, sep="\t")
        insWT = insWT[~insWT["is_bad_bin"]].drop(columns=["is_bad_bin"])
        insKO = pd.read_csv(input.insulation_KO, sep="\t")
        insKO = insKO[~insKO["is_bad_bin"]].drop(columns=["is_bad_bin"])
        ins = pd.merge(
            insWT, insKO, suffixes=("WT", "KO"), on=["chrom", "start", "end"]
        )
        diff_ins = ins[
            (  # Boundary much stronger in WT vs KO
                ins[f"boundary_strength_{wildcards.window}WT"]
                / ins[f"boundary_strength_{wildcards.window}KO"]
                >= config["compare_boundaries"]["fold_change_threshold"]
            )
            | (  # OR there is a strong boundary in WT and not in KO
                ins[f"is_boundary_{wildcards.window}WT"]
                & ~ins[f"is_boundary_{wildcards.window}KO"]
            )
        ]
        diff_ins[["chrom", "start", "end"]].to_csv(
            output[0], header=False, index=False, sep="\t"
        )


rule make_tads:
    input:
        insulation=(f"{insulation_folder}/{{sample}}_{{resolution}}.insulation.tsv"),
    output:
        f"{tad_folder}/TADs_{{sample}}_{{resolution,[0-9]+}}_{{window,[0-9]+}}.bed",
    threads: 1
    resources:
        mem_mb=1024,
        runtime=60,
    run:
        ins = pd.read_csv(input.insulation, sep="\t")
        tads = bioframe.merge(ins[ins[f"is_boundary_{wildcards.window}"] == False])

        tads = tads[
            (tads["end"] - tads["start"]) <= config["call_TADs"]["max_tad_length"]
        ].reset_index(drop=True)
        tads.to_csv(output[0], header=False, index=False, sep="\t")


rule make_insulation:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        view=lambda wildcards: config["view"],
    output:
        f"{insulation_folder}/{{sample}}_{{resolution,[0-9]+}}.insulation.tsv",
    params:
        window=lambda wildcards: config["insulation"]["resolutions"][
            int(wildcards.resolution)
        ],
        extra=lambda wildcards: config["insulation"].get("extra", ""),
    threads: 4
    resources:
        mem_mb=32 * 1024,
        runtime=240,
    wrapper:
        "v1.20.0/bio/cooltools/insulation"


include: "rules/pentads.smk"
include: "rules/dots.smk"
include: "rules/eigenvectors.smk"
include: "rules/insulation_tads.smk"
include: "rules/expected.smk"
include: "rules/saddle.smk"
include: "rules/pileups.smk"


rule get_cool_data:
    output:
        f"{coolers_folder}/{{sample}}.mcool",
    threads: 1
    resources:
        mem_mb=256,
        runtime=60,
    params:
        file=lambda wildcards: coollinks_dict[wildcards.sample],
    run:
        get_file(str(params.file), output[0])
        verify_view_cooler(
            cooler.Cooler(f"{output[0]}::resolutions/{config['resolutions'][0]}")
        )


rule get_bedpe_data:
    output:
        f"{bedpes_folder}/{{bedname}}.bedpe",
    threads: 1
    resources:
        mem_mb=256,
        runtime=60,
    params:
        file=lambda wildcards: bedlinks_dict[wildcards.bedname],
    run:
        get_file(str(params.file), str(output))


rule get_bed_data:
    output:
        f"{beds_folder}/{{bedname}}.bed",
    threads: 1
    resources:
        mem_mb=256,
        runtime=60,
    params:
        file=lambda wildcards: bedlinks_dict[wildcards.bedname],
    run:
        get_file(str(params.file), str(output))
