###### Import libraries #######
import os
from os import path
from glob import glob
import cooler
import numpy as np
import pandas as pd
import bioframe
from scipy import spatial


localrules:
    all,
    merge_dots_across_resolutions,
    make_differential_insulation,
    make_tads,
    call_loops_mustache,
    get_bed_data,
    get_cool_data,


###### Read config parameters #######

project_folder = config.get("project_folder", "results")
inputs_folder = config.get("inputs_folder", "inputs")
coolers_folder = config.get("coolers_folder", path.join(inputs_folder, "coolers"))
beds_folder = path.normpath(config.get("beds_folder", path.join(inputs_folder, "beds")))
beds_folder_name = path.basename(beds_folder)
bedpes_folder = path.normpath(
    config.get("bedpes_folder", path.join(inputs_folder, "bedpes"))
)
bedpes_folder_name = path.basename(bedpes_folder)
expected_folder = path.normpath(
    config.get("expected_folder", path.join(project_folder, "expected"))
)
pileups_folder = path.normpath(
    config.get("pileups_folder", path.join(project_folder, "pileups"))
)
eigenvectors_folder = path.normpath(
    config.get("eigenvectors_folder", path.join(project_folder, "eigenvectors"))
)
saddles_folder = path.normpath(
    config.get("saddles_folder", path.join(project_folder, "saddles"))
)
insulation_folder = path.normpath(
    config.get("insulation_folder", path.join(project_folder, "insulation"))
)
tad_folder = path.normpath(config.get("tad_folder", path.join(project_folder, "tads")))
tad_folder_name = path.basename(tad_folder)
loop_folder = path.normpath(
    config.get("loop_folder", path.join(project_folder, "loops"))
)
loop_folder_name = path.basename(loop_folder)
boundary_folder = path.normpath(
    config.get("boundary_folder", path.join(project_folder, "boundaries"))
)
boundary_folder_name = path.basename(boundary_folder)
outfolders = {
    tad_folder_name: tad_folder,
    loop_folder_name: loop_folder,
    boundary_folder_name: boundary_folder,
    beds_folder_name: beds_folder,
}

### Input genome
genome = config["genome"]
chroms = pd.read_csv(
    f'{config["chromsizes"]}', sep="\t", names=["chrom", "start", "end"]
)["chrom"].values

### Input cool files
from urllib.parse import urlparse


def will_download(link):
    parsed_path = urlparse(link)
    return False if parsed_path.scheme == "" else True


samples_df = pd.read_csv(config["samples"], sep="\t", header=0, comment="#")
samples_df.loc[:, "will_download"] = samples_df.file.apply(will_download)
samples_df.loc[:, "local_path"] = samples_df.apply(
    lambda x: f"{coolers_folder}/{x['sample']}.mcool" if x.will_download else x["file"],
    axis=1,
)
samples_df = samples_df.set_index("sample")

samples = list(samples_df.index)
coollinks_dict = dict(
    samples_df.query("will_download")["file"]
)  # dict with cools to be downloaded
coolfiles_dict = dict(samples_df["local_path"])

### Input bed files
def get_files(folder, extension):
    files = list(map(path.basename, glob(f"{folder}/*{extension}")))
    return files


def make_local_path(bedname, kind):
    if kind == "bed":
        return f"{beds_folder}/{bedname}.bed"
    elif kind == "bedpe":
        return f"{bedpes_folder}/{bedname}.bedpe"
    else:
        raise ValueError("Only bed and bedpe file types are supported")


bedfiles_local = get_files(beds_folder, "bed")
bedpefiles_local = get_files(bedpes_folder, "bedpe")

local_bed_names = {
    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local
}
local_bedpe_names = {
    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"
    for bedpefile in bedpefiles_local
}

bed_df = pd.read_csv(config["annotations"], sep="\t", header=0, comment="#")
bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)
bed_df.loc[:, "local_path"] = bed_df.apply(
    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,
    axis=1,
)
bed_df = bed_df.set_index("bedname").replace("-", np.nan)

pileup_params = config["pileups"]["arguments"]

bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()

bedlinks_dict = dict(
    bed_df.query("will_download")["file"]
)  # dict with beds to be downloaded
bedfiles_dict = dict(bed_df["local_path"])
bedfiles_dict.update(local_bed_names)
bedfiles_dict.update(local_bedpe_names)
bedfiles = list(bedfiles_dict.keys())
# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, 'pileups']]
bedtype_dict = dict(bed_df["format"])
# bedpe_pileups_mindist, bedpe_pileups_maxdist = config['bedpe_pileups_distance_limits']

samples_annotations = ~pd.read_csv(
    config["samples_annotations_combinations"],
    sep="\t",
    header=0,
    index_col=0,
    comment="#",
).isna()
### Data resolutions
ignore_resolutions_more_than = config["ignore_resolutions_more_than"]
resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers
minresolution = min(resolutions)
resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))

if config["eigenvector"]["do"]:
    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]
    eigenvector_resolutions = list(
        filter(
            lambda x: eigenvector_resolution_limits[0]
            <= x
            <= eigenvector_resolution_limits[1],
            resolutions,
        )
    )

if config["saddle"]["do"]:
    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]
    saddle_mindists = [
        int(saddle_mindist * 2**i)
        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))
    ]
    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]

if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:
    shifts = config["pileups"]["shifts"]
    pileup_norms = []
    if shifts > 0:
        pileup_norms.append(f"{shifts}-shifts")
    if config["pileups"]["expected"]:
        pileup_norms.append("expected")
    if len(pileup_norms) == 0:
        raise ValueError("Please use expected or shifts to normalize pileups")
    pileup_resolution_limits = config["pileups"]["resolution_limits"]
    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]
    pileup_resolutions = list(
        filter(
            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],
            resolutions,
        )
    )
    mindists = [
        int(pileups_mindist * 2**i)
        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))
    ]
    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]

if config["insulation"]["do"]:
    insul_res_win = []
    for resolution in config["insulation"]["resolutions"]:
        for win in config["insulation"]["resolutions"][resolution]:
            insul_res_win.append(f"{resolution}_{win}")

if config["call_TADs"]["do"]:
    tad_res_win = []
    for resolution in config["call_TADs"]["resolutions"]:
        for win in config["call_TADs"]["resolutions"][resolution]:
            tad_res_win.append(f"{resolution}_{win}")

# chroms = cooler.Cooler(f'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}').chroms()[:]

# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]
# bedpe_separations = [f'{mindist}-{mindist*2}' for mindist in bedpe_mindists]

expecteds = expand(
    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",
    sample=samples,
    resolution=resolutions,
)

diff_boundaries = (
    expand(
        f"{boundary_folder}/Insulation_{config['compare_boundaries']['samples'][0]}_not_"
        f"{config['compare_boundaries']['samples'][1]}_{{insul_res_win}}.bed",
        insul_res_win=insul_res_win,
    )
    if config["compare_boundaries"]["do"]
    else []
)
diff_boundaries_pileups = (
    expand(
        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config['compare_boundaries']['samples'][0]}_not_"
        f"{config['compare_boundaries']['samples'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",
        sample=samples,
        resolution=pileup_resolutions,
        insul_res_win=insul_res_win,
        norm=pileup_norms,
    )
    if config["compare_boundaries"]["do"] and config["pileups"]["do"]
    else []
)

tads = (
    expand(
        f"{tad_folder}/TADs_{{sampleTADs}}_{{tad_res_win}}.bed",
        sampleTADs=config["call_TADs"]["samples"],
        tad_res_win=tad_res_win,
    )
    if config["call_TADs"]["do"]
    else []
)

tads_pileups = (
    expand(
        f"{pileups_folder}/{tad_folder_name}/{{sample}}-{{resolution}}_over_TADs_{{sampleTADs}}_{{tad_res_win}}_{{norm}}_local_rescaled.clpy",
        sample=samples,
        resolution=pileup_resolutions,
        sampleTADs=config["call_TADs"]["samples"],
        tad_res_win=tad_res_win,
        norm=pileup_norms,
    )
    if config["call_TADs"]["do"] and config["pileups"]["do"]
    else []
)
dot_methods = [
    m for m in config["call_dots"]["methods"] if config["call_dots"]["methods"][m]["do"]
]
if dot_methods:
    loops = expand(
        f"{loop_folder}/merged_resolutions/Loops_{{method}}_{{sampleLoops}}.bedpe",
        method=dot_methods,
        sampleLoops=config["call_dots"]["samples"],
    )

    loops_pileups = (
        expand(
            f"{pileups_folder}/{loop_folder_name}/{{sample}}-{{resolution}}_over_Loops_{{method}}_{{sampleLoops}}_{{norm}}_{{mode}}.clpy",
            sample=samples,
            resolution=pileup_resolutions,
            method=dot_methods,
            sampleLoops=config["call_dots"]["samples"],
            norm=pileup_norms,
            mode=["distal", "by_distance"],
        )
        if config["pileups"]["do"]
        else []
    )
else:
    loops = []
    loops_pileups = []

for file in loops:
    name = path.splitext(path.basename(file))[0]
    bedfiles_dict[name] = file
    bedtype_dict[name] = "bedpe"
for file in tads + diff_boundaries:
    name = path.splitext(path.basename(file))[0]
    bedfiles_dict[name] = file
    bedtype_dict[name] = "bed"

beds_pileups = []
if config["pileups"]["do"]:
    for bedname, row in bed_df.iterrows():
        modes = []
        for mode in pileup_params.keys():
            if row[mode]:
                modes += [mode]

        for sample in samples:
            if sample not in samples_annotations.index:
                continue
            if (
                bedname in samples_annotations.columns
                and not samples_annotations.loc[sample, bedname]
            ):
                continue
            beds_pileups += expand(
                f"{pileups_folder}/{beds_folder_name}/{sample}-{{resolution}}_over_{bedname}_{{norm}}_{{mode}}.clpy",
                resolution=pileup_resolutions,
                norm=pileup_norms,
                mode=modes,
            )

saddles = (
    expand(
        f"{saddles_folder}/{{sample}}_{{resolution}}_{{bins}}{{dist}}.{{ending}}",
        sample=samples,
        resolution=eigenvector_resolutions,
        bins=config["saddle"]["bins"],
        dist=saddle_separations + [""],
        ending=["saddledump.npz", "digitized.tsv"],
    )
    if config["saddle"]["do"]
    else []
)


def split_dist(dist_wildcard, mindist_arg="--mindist", maxdist_arg="--maxdist"):
    if dist_wildcard == "":
        return ""
    else:
        assert dist_wildcard.startswith("_dist_")
        dists = dist_wildcard.split("_")[-1]
        mindist, maxdist = dists.split("-")
        return f"{mindist_arg} {mindist} {maxdist_arg} {maxdist}"


###### Define rules #######
rule all:
    input:
        lambda wildcards: expecteds,
        lambda wildcards: diff_boundaries,
        lambda wildcards: diff_boundaries_pileups,
        lambda wildcards: tads,
        lambda wildcards: tads_pileups,
        lambda wildcards: loops,
        lambda wildcards: loops_pileups,
        lambda wildcards: beds_pileups,
        lambda wildcards: saddles,


rule make_pileups:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        features=lambda wildcards: bedfiles_dict[wildcards.features],
        expected=f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",
        view=lambda wildcards: config["view"],
    output:
        f"{pileups_folder}/{{folder}}/{{sample}}-{{resolution,[0-9]+}}_over_{{features}}_expected_{{extra,.*}}.clpy",
    params:
        format=lambda wildcards: bedtype_dict[wildcards.features],
        ooe=lambda wildcards: "--ooe" if config["pileups"]["ooe"] else "",
        extra=lambda wildcards: pileup_params[wildcards.extra],
    threads: 4
    resources:
        mem_mb=lambda wildcards, threads: threads * 16 * 1024,
        runtime=24 * 60,
    shell:
        f"coolpup.py {{input.cooler}}::resolutions/{{wildcards.resolution}} {{input.features}} --view {{input.view}} --features_format {{params.format}} {{params.extra}} -p {{threads}} --expected {{input.expected}} --outname {{output[0]}}"


rule make_pileups_shifts:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        features=lambda wildcards: bedfiles_dict[wildcards.features],
        view=lambda wildcards: config["view"],
    output:
        f"{pileups_folder}/{{folder}}/{{sample}}-{{resolution,[0-9]+}}_over_{{features}}_{{shifts}}-shifts_{{mode,.*}}.clpy",
    params:
        format=lambda wildcards: bedtype_dict[wildcards.features],
        mode=lambda wildcards: pileup_params[wildcards.mode],
    threads: 8
    resources:
        mem_mb=lambda wildcards, threads: threads * 16 * 1024,
        runtime=24 * 60,
    shell:
        f"coolpup.py {{input.cooler}}::resolutions/{{wildcards.resolution}} {{input.features}} --view {{input.view}} --features_format {{params.format}} --nshifts {{shifts}} {{params.mode}} -p {{threads}} --outname {{output[0]}}"


def dedup_dots(dots, hiccups_filter=False):
    newdots = []
    ress = list(sorted(set(dots["res"])))
    for chrom in sorted(set(dots["chrom1"])):
        chromdots = (
            dots[dots["chrom1"] == chrom]
            .sort_values(["start1", "start2"])
            .reset_index(drop=True)
        )
        for res in ress:
            chromdots["Supported_%s" % res] = chromdots["res"] == res
        tree = spatial.cKDTree(chromdots[["start1", "start2"]])
        drop = []
        for i, j in tree.query_pairs(r=20000):
            ires = chromdots.at[i, "res"]
            jres = chromdots.at[j, "res"]
            chromdots.at[j, "Supported_%s" % ires] = True
            chromdots.at[i, "Supported_%s" % jres] = True
            if ires == jres:
                continue
            elif ires > jres:
                # if ress[-1] in (ires, jres) or abs(chromdots.at[j, 'start1']-chromdots.at[i, 'start1'])<=20000:
                drop.append(i)
            else:
                drop.append(j)
        newdots.append(chromdots.drop(drop))
    deduped = pd.concat(newdots).sort_values(["chrom1", "start1", "start2"])
    if hiccups_filter:
        l = len(deduped)
        deduped = deduped[
            ~(
                (deduped["start2"] - deduped["start1"] > 100000)
                & (~np.any(deduped[["Supported_%s" % res for res in ress[1:]]], axis=1))
            )
        ]
        print(
            l - len(deduped),
            "loops filtered out as unreliable %s resolution calls" % ress[0],
        )
    return deduped


def read_dots(f):
    df = bioframe.read_table(f, schema="bedpe", index_col=False).dropna(axis=1)
    res = int(f.split("_")[-1].split(".")[0])
    df["res"] = res
    return df


rule merge_dots_across_resolutions:
    input:
        dots=lambda wildcards,: [
            f"{loop_folder}/Loops_{{method}}_{{sample}}_{resolution}.bedpe"
            for resolution in config["call_dots"]["resolutions"]
        ],
    output:
        f"{loop_folder}/merged_resolutions/Loops_{{method}}_{{sample}}.bedpe",
    threads: 1
    resources:
        mem_mb=lambda wildcards, threads: 1024,
        runtime=5,
    run:
        dots = pd.concat((map(read_dots, input.dots))).reset_index(drop=True)
        dots = dedup_dots(dots)[
            ["chrom1", "start1", "end1", "chrom2", "start2", "end2"]
        ]
        dots.to_csv(output[0], sep="\t", header=False, index=False)


rule call_loops_cooltools:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        expected=f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",
        view=lambda wildcards: config["view"],
    output:
        f"{loop_folder}/Loops_cooltools_{{sample}}_{{resolution,[0-9]+}}.bedpe",
    threads: 4
    params:
        extra=lambda wildcards: config["call_dots"]["methods"]["cooltools"]["extra"],
    resources:
        mem_mb=lambda wildcards, threads: threads * 16 * 1024,
        runtime=24 * 60,
    wrapper:
        "v1.10.0/bio/cooltools/dots"


rule call_loops_chromosight:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
    output:
        f"{loop_folder}/Loops_chromosight_{{sample}}_{{resolution,[0-9]+}}.bedpe",
    threads: 4
    resources:
        mem_mb=lambda wildcards, threads: threads * 16 * 1024,
        runtime=24 * 60,
    shell:
        f"chromosight detect --pattern loops --no-plotting -t {{threads}} {{input.cooler}}::resolutions/{{wildcards.resolution}} {loop_folder}/loops_chromosight_{{wildcards.sample}}_{{wildcards.resolution}} && "
        f"tail -n +2 {loop_folder}/loops_chromosight_{{wildcards.sample}}_{{wildcards.resolution}}.tsv | cut -f1-6 > {{output}}"


rule call_loops_mustache_chrom:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
    output:
        temp(f"{loop_folder}/Loops_mustache_{{sample}}_{{resolution,[0-9]+}}.bedpe"),
    threads: 4
    params:
        args=config["call_dots"]["methods"]["mustache"]["extra"],
        dist=config["call_dots"]["methods"]["mustache"]["max_dist"],
    resources:
        mem_mb=lambda wildcards, threads: threads * 16 * 1024,
        runtime=24 * 60,
    conda:
        "envs/mustache_env.yml"
    shell:
        f"python3 -m mustache -p {{threads}} -f {{input.cooler}} -r {{wildcards.resolution}} "
        f"-d {{params.dist}} {{params.args}} -o {{output}}"


rule call_loops_mustache:
    input:
        lambda wildcards: [
            f"{loop_folder}/Mustache_bychr/Loops_mustache_{{sample}}_{{resolution,[0-9]+}}_{chrom}.bedpe"
            for chrom in chroms
        ],
    output:
        f"{loop_folder}/Loops_mustache_{{sample}}_{{resolution,[0-9]+}}.bedpe",
    resources:
        mem_mb=1024,
        runtime=60,
    shell:
        f"awk 'FNR-1' {{input}} | cut -f1-6 > {{output}}"


rule make_differential_insulation:
    input:
        insulation_WT=(
            f"{insulation_folder}/{{sampleWT}}_{{resolution}}_{{window}}.insulation.tsv"
        ),
        insulation_KO=(
            f"{insulation_folder}/{{sampleKO}}_{{resolution}}_{{window}}.insulation.tsv"
        ),
    output:
        f"{boundary_folder}/Insulation_{{sampleWT}}_not_{{sampleKO}}_{{resolution,[0-9]+}}_{{window,[0-9]+}}.bed",
    threads: 1
    resources:
        mem_mb=1024,
        runtime=60,
    run:
        from skimage.filters import threshold_li, threshold_otsu

        if config["insulation"]["strength_threshold"] == "Li":
            thresholding_fun = threshold_li
        elif config["insulation"]["strength_threshold"] == "Otsu":
            thresholding_fun = threshold_otsu
        else:
            try:
                thr = float(config["insulation"]["strength_threshold"])
                thresholding_fun = lambda x: thr
            except ValueError:
                raise ValueError(
                    "Insulating boundary strength threshold can be Li, Otsu or a float"
                )
        insWT = pd.read_csv(input.insulation_WT, sep="\t")
        insWT = insWT[~insWT["is_bad_bin"]].drop(columns=["is_bad_bin"])
        insKO = pd.read_csv(input.insulation_KO, sep="\t")
        insKO = insKO[~insKO["is_bad_bin"]].drop(columns=["is_bad_bin"])
        ins = pd.merge(
            insWT, insKO, suffixes=("WT", "KO"), on=["chrom", "start", "end"]
        )
        diff_ins = ins[
            (
                ins[f"boundary_strength_{wildcards.window}WT"]
                / ins[f"boundary_strength_{wildcards.window}KO"]
                >= config["compare_boundaries"]["fold_change_threshold"]
            )
            | (
                (
                    ins[f"boundary_strength_{wildcards.window}WT"]
                    > thresholding_fun(
                        insWT[f"boundary_strength_{wildcards.window}"].dropna().values
                    )
                )
                & np.isnan(ins[f"boundary_strength_{wildcards.window}KO"])
            )
        ]
        diff_ins[["chrom", "start", "end"]].to_csv(
            output[0], header=False, index=False, sep="\t"
        )


rule make_tads:
    input:
        insulation=(f"{insulation_folder}/{{sample}}_{{resolution}}.insulation.tsv"),
    output:
        f"{tad_folder}/TADs_{{sample}}_{{resolution,[0-9]+}}_{{window,[0-9]+}}.bed",
    threads: 1
    resources:
        mem_mb=1024,
        runtime=60,
    run:
        ins = pd.read_csv(input.insulation, sep="\t")
        tads = bioframe.merge(ins[ins[f"is_boundary_{wildcards.window}"] == False])

        tads = tads[
            (tads["end"] - tads["start"]) <= config["call_TADs"]["max_tad_length"]
        ].reset_index(drop=True)
        tads.to_csv(output[0], header=False, index=False, sep="\t")


rule make_insulation:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        view=lambda wildcards: config["view"],
    output:
        f"{insulation_folder}/{{sample}}_{{resolution,[0-9]+}}.insulation.tsv",
    params:
        window=lambda wildcards: config["insulation"]["resolutions"][
            int(wildcards.resolution)
        ],
        extra=lambda wildcards: config["insulation"].get("extra", ""),
    threads: 1
    resources:
        mem_mb=32 * 1024,
        runtime=240,
    wrapper:
        "v1.10.0/bio/cooltools/insulation"


rule make_saddles:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        track=(
            f"{eigenvectors_folder}/{{sample}}_{{resolution}}_eigenvectors.cis.vecs.tsv"
        ),
        expected=f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",
        view=lambda wildcards: config["view"],
    output:
        saddle=f"{saddles_folder}/{{sample}}_{{resolution,[0-9]+}}_{{bins,[0-9]+}}{{dist,.*}}.saddledump.npz",
        digitized_track=f"{saddles_folder}/{{sample}}_{{resolution,[0-9]+}}_{{bins,[0-9]+}}{{dist,.*}}.digitized.tsv",
    params:
        extra=lambda wildcards: " ".join(
            [
                config["saddle"]["extra"],
                split_dist(wildcards.dist, "--min-dist", "--max-dist"),
                "--n-bins {wildcards.bins}",
            ]
        ),
        range=lambda wildcards: config["saddle"]["range"],
    threads: 1
    resources:
        mem_mb=8 * 1024,
        runtime=60,
    wrapper:
        "v1.10.0/bio/cooltools/saddle"


rule make_expected_cis:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        view=lambda wildcards: config["view"],
    output:
        f"{expected_folder}/{{sample}}_{{resolution,[0-9]+}}.expected.tsv",
    params:
        extra="--ignore-diags 0",
    threads: 4
    resources:
        mem_mb=lambda wildcards, threads: threads * 8 * 1024,
        runtime=60,
    wrapper:
        "v1.10.0/bio/cooltools/expected_cis"


rule make_eigenvectors_cis:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        view=lambda wildcards: config["view"],
        track=lambda wildcards: path.join(
            config["path_genome_folder"],
            "gc/",
            f"{genome}_{{resolution}}_gc.bedgraph",
        ),
    output:
        vecs=f"{eigenvectors_folder}/{{sample}}_{{resolution,[0-9]+}}_eigenvectors.cis.vecs.tsv",
        lam=f"{eigenvectors_folder}/{{sample}}_{{resolution,[0-9]+}}_eigenvectors.cis.lam.txt",
        bigwig=f"{eigenvectors_folder}/{{sample}}_{{resolution,[0-9]+}}_eigenvectors.cis.bw",
    params:
        extra="--bigwig",
    threads: 1
    resources:
        mem_mb=8 * 1024,
        runtime=60,
    wrapper:
        "v1.10.0/bio/cooltools/eigs_cis"


rule make_eigenvectors_trans:
    input:
        cooler=lambda wildcards: coolfiles_dict[wildcards.sample],
        track=lambda wildcards: path.join(
            config["path_genome_folder"],
            "gc/",
            f"{genome}_{{resolution}}_gc.bedgraph",
        ),
    output:
        f"{eigenvectors_folder}/{{sample}}_{{resolution,[0-9]+}}_eigenvectors.trans.vecs.tsv",
        f"{eigenvectors_folder}/{{sample}}_{{resolution,[0-9]+}}_eigenvectors.trans.lam.txt",
        f"{eigenvectors_folder}/{{sample}}_{{resolution,[0-9]+}}_eigenvectors.trans.bw",
    params:
        track_name_col="GC",
        extra="",
    threads: 1
    resources:
        mem_mb=8 * 1024,
        runtime=60,
    wrapper:
        "v1.10.0/bio/cooltools/eigs_trans"


rule make_gc:
    input:
        fasta=config["path_genome_fasta"],
        bins=lambda wildcards: path.join(
            f'{config["path_genome_folder"]}',
            "bins/",
            f"{genome}_{{resolution}}_bins.bed",
        ),
    output:
        path.join(
            config["path_genome_folder"],
            "gc/",
            f"{genome}_{{resolution,[0-9]+}}_gc.bedgraph",
        ),
    threads: 1
    resources:
        mem_mb=8 * 1024,
        runtime=60,
    shell:
        "v1.10.0/bio/cooltools/genome/gc"


rule make_bins:
    input:
        chromsizes=config["chromsizes"],
    output:
        path.join(
            config["path_genome_folder"],
            "bins/",
            f"{genome}_{{resolution,[0-9]+}}_bins.bed",
        ),
    threads: 1
    resources:
        mem_mb=8 * 1024,
        runtime=60,
    params:
        binsize=lambda wildcards: wildcards["resolution"],
    wrapper:
        "v1.10.0/bio/cooltools/genome/binnify"


def download_file(file, local_filename):
    import requests
    import tqdm
    import re

    with requests.get(file, stream=True) as r:
        ext_gz = (
            ".gz"
            if re.findall("filename=(.+)", r.headers["content-disposition"])[
                0
            ].endswith(".gz")
            else ""
        )
        r.raise_for_status()
        print("downloading:", file, "as ", local_filename + ext_gz)
        with open(local_filename + ext_gz, "wb") as f:
            for chunk in tqdm.tqdm(r.iter_content(chunk_size=8192)):
                f.write(chunk)
    return local_filename + ext_gz


def get_file(file, output):
    """
    If input is URL, it download via python's requests. Uncompress if needed.
    """
    if file == output:
        exit()

    from urllib.parse import urlparse

    parsed_path = urlparse(file)
    if parsed_path.scheme == "http" or parsed_path.scheme == "https":
        output_file = download_file(file, output)
    else:
        raise Exception(
            f"Unable to download from: {file}\nScheme {parsed_url.scheme} is not supported"
        )
    if output_file.endswith(".gz"):
        shell(f"gzip -d {output_file}")


rule get_cool_data:
    output:
        f"{coolers_folder}/{{sample}}.mcool",
    threads: 1
    resources:
        mem_mb=256,
        runtime=60,
    params:
        file=lambda wildcards: coollinks_dict[wildcards.sample],
    run:
        get_file(str(params.file), str(output))


rule get_bedpe_data:
    output:
        f"{bedpes_folder}/{{bedname}}.bedpe",
    threads: 1
    resources:
        mem_mb=256,
        runtime=60,
    params:
        file=lambda wildcards: bedlinks_dict[wildcards.bedname],
    run:
        get_file(str(params.file), str(output))


rule get_bed_data:
    output:
        f"{beds_folder}/{{bedname}}.bed",
    threads: 1
    resources:
        mem_mb=256,
        runtime=60,
    params:
        file=lambda wildcards: bedlinks_dict[wildcards.bedname],
    run:
        get_file(str(params.file), str(output))
